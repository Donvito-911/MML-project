{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a6a8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:08.902683933Z",
     "start_time": "2023-07-25T13:01:07.536915871Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 08:01:07.677018: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 08:01:08.166991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Concatenate, Conv2D, Conv2DTranspose, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(\"/home/danielaguirre/Documents/university/masteringml/MML-project/data/processed_data.h5\", \"r\") as f:\n",
    "    imgs_train, labels_train = np.array(f[\"X_train\"]), np.array(f[\"y_train\"])\n",
    "    imgs_test, labels_test = np.array(f[\"X_test\"]), np.array(f[\"y_test\"])\n",
    "\n",
    "imgs_train2 = []\n",
    "imgs_test2 = []\n",
    "for img in imgs_train:\n",
    "    new_img = cv2.resize(img, (128,128))\n",
    "    new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "    imgs_train2.append(new_img)\n",
    "for img in imgs_test:\n",
    "    new_img = cv2.resize(img, (128,128))\n",
    "    new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "    imgs_test2.append(new_img)\n",
    "imgs_train = np.array(imgs_train2)[:, :, :, np.newaxis]\n",
    "imgs_test = np.array(imgs_test2)[:, :, :, np.newaxis]\n",
    "imgs_train2 = (imgs_train-127.5)/127.5\n",
    "imgs_test2 = (imgs_test-127.5)/127.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:09.265948257Z",
     "start_time": "2023-07-25T13:01:08.904418157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size = 128 # batch size\n",
    "latent_dim = 256 # latent space size\n",
    "optim = Adam(lr=0.0001)\n",
    "shape_img = imgs_train.shape[1:]\n",
    "n_cat = labels_train.shape[1]\n",
    "n_epoch = 300"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:09.269019898Z",
     "start_time": "2023-07-25T13:01:09.267719510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb75caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:11.544406224Z",
     "start_time": "2023-07-25T13:01:09.269835781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 08:01:09.351826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.363769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.363898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/danielaguirre/miniconda3/envs/ds/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 08:01:09.370613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.370719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.370805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.873254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.873379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.873472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:09.873563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22064 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet50(weights=None, include_top=False, input_shape=(128,128,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5dfc5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:12.680806073Z",
     "start_time": "2023-07-25T13:01:11.547017879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_image (InputLayer)    [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " resnet50 (Functional)       (None, 4, 4, 2048)           2358144   ['input_image[0][0]']         \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 4, 4, 32)             589856    ['resnet50[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 4, 4, 64)             18496     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 4, 4, 64)             0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)            73856     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 4, 4, 128)            0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 256)            295168    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 512)            1180160   ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 8192)                 0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 8196)                 0         ['flatten[0][0]',             \n",
      "                                                                     'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " enc_hidden_layer (Dense)    (None, 1048)                 8590456   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 1048)                 0         ['enc_hidden_layer[0][0]']    \n",
      "                                                                                                  \n",
      " enc_hidden_layer2 (Dense)   (None, 512)                  537088    ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " mu (Dense)                  (None, 256)                  131328    ['enc_hidden_layer2[0][0]']   \n",
      "                                                                                                  \n",
      " l_sigma (Dense)             (None, 256)                  131328    ['enc_hidden_layer2[0][0]']   \n",
      "                                                                                                  \n",
      " latent_vector (Lambda)      (None, 256)                  0         ['mu[0][0]',                  \n",
      "                                                                     'l_sigma[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 35129176 (134.01 MB)\n",
      "Trainable params: 35076056 (133.80 MB)\n",
      "Non-trainable params: 53120 (207.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_input_image = Input(shape=(shape_img), name=\"input_image\")\n",
    "enc_input_label = Input(shape=(n_cat), name=\"input_label\")\n",
    "\n",
    "enc = resnet(enc_input_image)\n",
    "# convolutionals block\n",
    "enc_conv1 = Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\")(enc) # 32x32\n",
    "enc_conv2 = Conv2D(filters=64, kernel_size=3, padding=\"same\", activation='relu')(enc_conv1) #  skip-connection 16x16\n",
    "drop_out = Dropout(rate=0.3)(enc_conv2)\n",
    "enc_conv3 = Conv2D(filters=128, kernel_size=3, padding=\"same\", activation='relu')(drop_out) # 8x8\n",
    "drop_out = Dropout(rate=0.3)(enc_conv3)\n",
    "enc_conv4 = Conv2D(filters=256, kernel_size=3, padding=\"same\", activation='relu')(drop_out)  #  skip-connection # 16x16\n",
    "enc_conv5 = Conv2D(filters=512, kernel_size=3, padding=\"same\", activation='relu')(enc_conv4)  #  skip-connection # 16x16\n",
    "\n",
    "# concat\n",
    "flattened = Flatten()(enc_conv5)\n",
    "enc_concat = Concatenate()([flattened, enc_input_label])\n",
    "\n",
    "encoder_hidden = Dense(1048, name=\"enc_hidden_layer\")(enc_concat)\n",
    "drop_out = Dropout(rate=0.3)(encoder_hidden)\n",
    "encoder_hidden = Dense(512, name=\"enc_hidden_layer2\")(drop_out)\n",
    "\n",
    "mu = Dense(latent_dim, activation='linear', name=\"mu\")(encoder_hidden)\n",
    "l_sigma = Dense(latent_dim, activation='linear', name=\"l_sigma\")(encoder_hidden)\n",
    "def sample_z(args):\n",
    "    mu, l_sigma = args\n",
    "    eps = K.random_normal(shape=(latent_dim, ), mean=0., stddev=1.) # shape antes => (batch_size, latent_dim)\n",
    "    return mu + K.exp(l_sigma / 2) * eps\n",
    "z = Lambda(sample_z, output_shape = (latent_dim, ), name=\"latent_vector\")([mu, l_sigma]) # output encoder\n",
    "\n",
    "encoder = Model([enc_input_image, enc_input_label], z, name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_latent_vector (Input  [(None, 256)]                0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " decoder_concat (Concatenat  (None, 260)                  0         ['input_latent_vector[0][0]', \n",
      " e)                                                                  'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " hidden_layer1 (Dense)       (None, 512)                  133632    ['decoder_concat[0][0]']      \n",
      "                                                                                                  \n",
      " hidden_layer2 (Dense)       (None, 512)                  262656    ['hidden_layer1[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 512)                  0         ['hidden_layer2[0][0]']       \n",
      "                                                                                                  \n",
      " hidden_layer3 (Dense)       (None, 512)                  262656    ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " hidden_layer4 (Dense)       (None, 512)                  262656    ['hidden_layer3[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 512)                  0         ['hidden_layer4[0][0]']       \n",
      "                                                                                                  \n",
      " hidden_layer5 (Dense)       (None, 512)                  262656    ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " hidden_layer6 (Dense)       (None, 1048)                 537624    ['hidden_layer5[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 1048)                 0         ['hidden_layer6[0][0]']       \n",
      "                                                                                                  \n",
      " hidden_layer7 (Dense)       (None, 8192)                 8593408   ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 16, 16, 32)           0         ['hidden_layer7[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 512)          147968    ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 256)          1179904   ['conv2d_5[0][0]']            \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 16, 16, 256)          0         ['conv2d_transpose[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 128)          295040    ['dropout_6[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 64)           73792     ['conv2d_transpose_1[0][0]']  \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 64, 64, 64)           0         ['conv2d_transpose_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 128, 128, 32)         18464     ['dropout_7[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2D  (None, 128, 128, 1)          289       ['conv2d_transpose_3[0][0]']  \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 16384)                0         ['conv2d_transpose_4[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12030745 (45.89 MB)\n",
      "Trainable params: 12030745 (45.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec_inp_latent_vector = Input(shape=(latent_dim), name=\"input_latent_vector\")\n",
    "decoder_inp_label = Input(shape=(n_cat), name = \"input_label\")\n",
    "dec_concat = Concatenate(name=\"decoder_concat\")([dec_inp_latent_vector, decoder_inp_label])\n",
    "\n",
    "decoder_hidden = Dense(512, activation=\"relu\", name=\"hidden_layer1\")(dec_concat)\n",
    "decoder_hidden = Dense(512, activation=\"relu\", name=\"hidden_layer2\")(decoder_hidden)\n",
    "decoder_hidden = Dropout(0.3)(decoder_hidden)\n",
    "decoder_hidden = Dense(512, activation=\"relu\", name=\"hidden_layer3\")(decoder_hidden)\n",
    "decoder_hidden = Dense(512, activation=\"relu\", name=\"hidden_layer4\")(decoder_hidden)\n",
    "decoder_hidden = Dropout(0.3)(decoder_hidden)\n",
    "decoder_hidden = Dense(512, activation=\"relu\", name=\"hidden_layer5\")(decoder_hidden)\n",
    "decoder_hidden = Dense(1048, activation=\"relu\", name=\"hidden_layer6\")(decoder_hidden)\n",
    "decoder_hidden = Dropout(rate=0.3)(decoder_hidden)\n",
    "decoder_hidden = Dense(16*16*32, activation=\"relu\", name=\"hidden_layer7\")(decoder_hidden)\n",
    "reshaped = Reshape(target_shape=(16,16,32))(decoder_hidden)\n",
    "\n",
    "# convolutionals block\n",
    "dec_conv1 = Conv2D(filters=512, kernel_size=3, padding=\"same\", activation='relu')(reshaped)  #  skip-connection # 16x16\n",
    "dec_conv1 = Conv2DTranspose(filters=256, kernel_size=3, activation=\"relu\", padding='same')(dec_conv1)\n",
    "dec_conv1 = Dropout(rate=0.3)(dec_conv1)\n",
    "dec_conv2 = Conv2DTranspose(filters=128, kernel_size=3, activation=\"relu\", strides=2, padding='same')(dec_conv1)\n",
    "dec_conv3 = Conv2DTranspose(filters=64, kernel_size=3, activation=\"relu\", strides=2, padding='same')(dec_conv2)\n",
    "dec_conv3 = Dropout(rate=0.3)(dec_conv3)\n",
    "dec_conv4 = Conv2DTranspose(filters=32, kernel_size=3, activation=\"relu\", strides=2, padding='same')(dec_conv3)\n",
    "\n",
    "dec_output_img = Conv2DTranspose(filters=1, kernel_size=3, padding=\"same\", activation=\"sigmoid\")(dec_conv4)\n",
    "dec_output_img = Flatten()(dec_output_img)\n",
    "decoder = Model([dec_inp_latent_vector, decoder_inp_label], dec_output_img, name=\"decoder\")\n",
    "decoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:12.942503114Z",
     "start_time": "2023-07-25T13:01:12.679620314Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CVAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_image (InputLayer)    [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        (None, 256)                  3512917   ['input_image[0][0]',         \n",
      "                                                          6          'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 16384)                1203074   ['encoder[0][0]',             \n",
      "                                                          5          'input_label[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47159921 (179.90 MB)\n",
      "Trainable params: 47106801 (179.70 MB)\n",
      "Non-trainable params: 53120 (207.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_vector = encoder(inputs=[enc_input_image, enc_input_label]) # z\n",
    "output_img = decoder(inputs=[latent_vector, enc_input_label])\n",
    "CVAE = Model(inputs=[enc_input_image, enc_input_label], outputs=output_img, name=\"CVAE\")\n",
    "CVAE.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:14.069339252Z",
     "start_time": "2023-07-25T13:01:12.976172041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    recon = recon_loss(y_true, y_pred)\n",
    "    kl = KL_loss(y_true, y_pred)\n",
    "    return recon + kl\n",
    "\n",
    "def KL_loss(y_true, y_pred): # it doesnt use y_true and y_pred but the parameters are necessary for compiling\n",
    "     # importante se usan las capas de mu y sigma del encoder\n",
    "    return(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))\n",
    "\n",
    "def recon_loss(y_true, y_pred):\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:14.069532290Z",
     "start_time": "2023-07-25T13:01:14.068352891Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "CVAE.compile(optimizer=optim, loss=vae_loss, metrics=[KL_loss, recon_loss])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:14.145260024Z",
     "start_time": "2023-07-25T13:01:14.068607009Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2937, 16384)\n"
     ]
    }
   ],
   "source": [
    "# the output of the decoder is a flattened img,\n",
    "# so we need to flatten the true values (input images from (28,28,1) => 784)\n",
    "y_train = imgs_train.reshape(imgs_train.shape[0], -1)\n",
    "y_test = imgs_test.reshape(imgs_test.shape[0], -1)\n",
    "print(y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T13:01:14.147783451Z",
     "start_time": "2023-07-25T13:01:14.146235605Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2937 samples, validate on 327 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 08:01:18.947466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:18.947613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:18.947758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:18.947949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:18.948094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-25 08:01:18.948195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22064 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:2b:00.0, compute capability: 8.6\n",
      "2023-07-25 08:01:19.217770: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-07-25 08:01:20.370995: W tensorflow/c/c_api.cc:304] Operation '{name:'training/Adam/conv4_block1_2_conv/kernel/v/Assign' id:13584 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv4_block1_2_conv/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv4_block1_2_conv/kernel/v, training/Adam/conv4_block1_2_conv/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 08:01:25.070793: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape indropout/cond/then/_2968/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-07-25 08:01:28.799917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-07-25 08:01:29.510855: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-07-25 08:01:29.511269: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-07-25 08:01:29.511280: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-07-25 08:01:29.511696: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-07-25 08:01:29.511731: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-07-25 08:01:29.640892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2937/2937 [==============================] - ETA: 0s - loss: 10454.7431 - KL_loss: 5.4056 - recon_loss: 10449.3379"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 08:01:44.977670: W tensorflow/c/c_api.cc:304] Operation '{name:'loss/mul' id:8414 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-07-25 08:01:46.066110: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape indropout/cond/then/_2968/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2937/2937 [==============================] - 24s 8ms/sample - loss: 10454.7431 - KL_loss: 5.4056 - recon_loss: 10449.3379 - val_loss: 8643.2126 - val_KL_loss: 0.0989 - val_recon_loss: 8643.1133\n",
      "Epoch 2/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7961.3712 - KL_loss: 0.7805 - recon_loss: 7960.5918 - val_loss: 7594.7330 - val_KL_loss: 0.3510 - val_recon_loss: 7594.3823\n",
      "Epoch 3/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7258.7036 - KL_loss: 39.1164 - recon_loss: 7219.5874 - val_loss: 7439.0857 - val_KL_loss: 2.5339 - val_recon_loss: 7436.5518\n",
      "Epoch 4/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7138.6866 - KL_loss: 38.7641 - recon_loss: 7099.9224 - val_loss: 7445.3733 - val_KL_loss: 4.1896 - val_recon_loss: 7441.1836\n",
      "Epoch 5/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7014.4213 - KL_loss: 32.0380 - recon_loss: 6982.3833 - val_loss: 7258.3702 - val_KL_loss: 8.7859 - val_recon_loss: 7249.5840\n",
      "Epoch 6/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7021.7695 - KL_loss: 38.2749 - recon_loss: 6983.4941 - val_loss: 7258.3940 - val_KL_loss: 5.4994 - val_recon_loss: 7252.8945\n",
      "Epoch 7/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7003.5089 - KL_loss: 27.7140 - recon_loss: 6975.7949 - val_loss: 7265.2363 - val_KL_loss: 5.2647 - val_recon_loss: 7259.9712\n",
      "Epoch 8/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6951.1404 - KL_loss: 28.6478 - recon_loss: 6922.4922 - val_loss: 7203.4642 - val_KL_loss: 3.6479 - val_recon_loss: 7199.8164\n",
      "Epoch 9/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6952.3535 - KL_loss: 28.1351 - recon_loss: 6924.2188 - val_loss: 7702.1498 - val_KL_loss: 2.3517 - val_recon_loss: 7699.7983\n",
      "Epoch 10/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6932.3094 - KL_loss: 26.7784 - recon_loss: 6905.5308 - val_loss: 7575.4201 - val_KL_loss: 1.8332 - val_recon_loss: 7573.5864\n",
      "Epoch 11/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6933.9344 - KL_loss: 25.7543 - recon_loss: 6908.1807 - val_loss: 7196.7279 - val_KL_loss: 3.8659 - val_recon_loss: 7192.8623\n",
      "Epoch 12/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7000.9690 - KL_loss: 22.1260 - recon_loss: 6978.8423 - val_loss: 7322.7179 - val_KL_loss: 2.6469 - val_recon_loss: 7320.0713\n",
      "Epoch 13/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6967.1049 - KL_loss: 16.8597 - recon_loss: 6950.2456 - val_loss: 7345.4923 - val_KL_loss: 3.2645 - val_recon_loss: 7342.2280\n",
      "Epoch 14/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6918.0659 - KL_loss: 20.3246 - recon_loss: 6897.7407 - val_loss: 7123.2635 - val_KL_loss: 4.8269 - val_recon_loss: 7118.4375\n",
      "Epoch 15/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6896.0498 - KL_loss: 18.0878 - recon_loss: 6877.9619 - val_loss: 7111.5607 - val_KL_loss: 4.6428 - val_recon_loss: 7106.9175\n",
      "Epoch 16/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6884.1997 - KL_loss: 17.5756 - recon_loss: 6866.6235 - val_loss: 7081.1304 - val_KL_loss: 4.6588 - val_recon_loss: 7076.4717\n",
      "Epoch 17/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 7001.4140 - KL_loss: 13.8257 - recon_loss: 6987.5879 - val_loss: 6829.1312 - val_KL_loss: 13.0726 - val_recon_loss: 6816.0591\n",
      "Epoch 18/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6924.8449 - KL_loss: 13.7045 - recon_loss: 6911.1406 - val_loss: 6755.3296 - val_KL_loss: 13.8573 - val_recon_loss: 6741.4727\n",
      "Epoch 19/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6929.0037 - KL_loss: 14.9715 - recon_loss: 6914.0327 - val_loss: 6944.5547 - val_KL_loss: 16.1734 - val_recon_loss: 6928.3813\n",
      "Epoch 20/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6867.3323 - KL_loss: 14.2882 - recon_loss: 6853.0439 - val_loss: 7080.5157 - val_KL_loss: 10.2248 - val_recon_loss: 7070.2905\n",
      "Epoch 21/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6867.4025 - KL_loss: 13.8839 - recon_loss: 6853.5186 - val_loss: 7001.6228 - val_KL_loss: 10.0164 - val_recon_loss: 6991.6064\n",
      "Epoch 22/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6883.2855 - KL_loss: 14.1848 - recon_loss: 6869.1006 - val_loss: 6804.3601 - val_KL_loss: 10.0445 - val_recon_loss: 6794.3159\n",
      "Epoch 23/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6861.3757 - KL_loss: 14.5796 - recon_loss: 6846.7959 - val_loss: 6812.8823 - val_KL_loss: 10.9487 - val_recon_loss: 6801.9336\n",
      "Epoch 24/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6819.5763 - KL_loss: 15.8122 - recon_loss: 6803.7637 - val_loss: 6821.9773 - val_KL_loss: 9.8934 - val_recon_loss: 6812.0835\n",
      "Epoch 25/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6864.5673 - KL_loss: 14.0005 - recon_loss: 6850.5664 - val_loss: 6807.1902 - val_KL_loss: 8.7884 - val_recon_loss: 6798.4014\n",
      "Epoch 26/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6835.2938 - KL_loss: 15.4072 - recon_loss: 6819.8862 - val_loss: 6814.2798 - val_KL_loss: 8.2702 - val_recon_loss: 6806.0093\n",
      "Epoch 27/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6814.5001 - KL_loss: 16.2406 - recon_loss: 6798.2593 - val_loss: 6929.9632 - val_KL_loss: 11.3703 - val_recon_loss: 6918.5933\n",
      "Epoch 28/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6816.0980 - KL_loss: 15.8634 - recon_loss: 6800.2358 - val_loss: 6810.3371 - val_KL_loss: 12.2454 - val_recon_loss: 6798.0918\n",
      "Epoch 29/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6825.9534 - KL_loss: 16.6575 - recon_loss: 6809.2954 - val_loss: 6963.7196 - val_KL_loss: 14.9306 - val_recon_loss: 6948.7891\n",
      "Epoch 30/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6835.9577 - KL_loss: 17.4973 - recon_loss: 6818.4609 - val_loss: 6884.6864 - val_KL_loss: 12.5117 - val_recon_loss: 6872.1743\n",
      "Epoch 31/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6824.3472 - KL_loss: 17.8094 - recon_loss: 6806.5381 - val_loss: 7073.9113 - val_KL_loss: 14.3816 - val_recon_loss: 7059.5288\n",
      "Epoch 32/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6810.7772 - KL_loss: 17.4647 - recon_loss: 6793.3120 - val_loss: 6767.4073 - val_KL_loss: 8.3883 - val_recon_loss: 6759.0190\n",
      "Epoch 33/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6853.3637 - KL_loss: 17.0049 - recon_loss: 6836.3584 - val_loss: 7000.5048 - val_KL_loss: 8.4979 - val_recon_loss: 6992.0068\n",
      "Epoch 34/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6809.9712 - KL_loss: 16.8390 - recon_loss: 6793.1333 - val_loss: 7008.8758 - val_KL_loss: 7.7833 - val_recon_loss: 7001.0923\n",
      "Epoch 35/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6814.8737 - KL_loss: 16.3255 - recon_loss: 6798.5474 - val_loss: 6820.2601 - val_KL_loss: 10.5318 - val_recon_loss: 6809.7285\n",
      "Epoch 36/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6816.5015 - KL_loss: 16.9006 - recon_loss: 6799.5996 - val_loss: 6992.0920 - val_KL_loss: 10.9679 - val_recon_loss: 6981.1240\n",
      "Epoch 37/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6805.4824 - KL_loss: 18.1411 - recon_loss: 6787.3403 - val_loss: 6911.0693 - val_KL_loss: 11.3246 - val_recon_loss: 6899.7446\n",
      "Epoch 38/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6791.3554 - KL_loss: 15.4803 - recon_loss: 6775.8755 - val_loss: 6884.7683 - val_KL_loss: 9.6229 - val_recon_loss: 6875.1450\n",
      "Epoch 39/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6808.7137 - KL_loss: 15.2729 - recon_loss: 6793.4409 - val_loss: 6822.5057 - val_KL_loss: 10.4730 - val_recon_loss: 6812.0327\n",
      "Epoch 40/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6852.1284 - KL_loss: 17.5919 - recon_loss: 6834.5366 - val_loss: 6927.8373 - val_KL_loss: 12.5324 - val_recon_loss: 6915.3052\n",
      "Epoch 41/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6823.7247 - KL_loss: 16.3625 - recon_loss: 6807.3613 - val_loss: 6828.8857 - val_KL_loss: 9.9580 - val_recon_loss: 6818.9282\n",
      "Epoch 42/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6791.4371 - KL_loss: 17.0279 - recon_loss: 6774.4087 - val_loss: 6847.2013 - val_KL_loss: 9.0425 - val_recon_loss: 6838.1592\n",
      "Epoch 43/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6786.6412 - KL_loss: 15.7835 - recon_loss: 6770.8574 - val_loss: 6792.9042 - val_KL_loss: 7.1178 - val_recon_loss: 6785.7866\n",
      "Epoch 44/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6816.0728 - KL_loss: 16.6041 - recon_loss: 6799.4683 - val_loss: 7280.0674 - val_KL_loss: 7.7131 - val_recon_loss: 7272.3550\n",
      "Epoch 45/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6849.3166 - KL_loss: 16.1173 - recon_loss: 6833.1997 - val_loss: 6771.8703 - val_KL_loss: 12.4008 - val_recon_loss: 6759.4692\n",
      "Epoch 46/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6784.3883 - KL_loss: 18.2864 - recon_loss: 6766.1016 - val_loss: 6834.3769 - val_KL_loss: 10.7633 - val_recon_loss: 6823.6133\n",
      "Epoch 47/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6803.1118 - KL_loss: 18.7234 - recon_loss: 6784.3882 - val_loss: 7171.8686 - val_KL_loss: 12.3363 - val_recon_loss: 7159.5327\n",
      "Epoch 48/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6781.0903 - KL_loss: 17.7020 - recon_loss: 6763.3887 - val_loss: 6806.3682 - val_KL_loss: 12.0090 - val_recon_loss: 6794.3594\n",
      "Epoch 49/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6783.4103 - KL_loss: 18.6927 - recon_loss: 6764.7173 - val_loss: 6733.7923 - val_KL_loss: 12.5464 - val_recon_loss: 6721.2461\n",
      "Epoch 50/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6802.4125 - KL_loss: 20.2804 - recon_loss: 6782.1333 - val_loss: 6917.4080 - val_KL_loss: 15.5630 - val_recon_loss: 6901.8457\n",
      "Epoch 51/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6809.3473 - KL_loss: 19.2761 - recon_loss: 6790.0723 - val_loss: 6861.5299 - val_KL_loss: 13.6099 - val_recon_loss: 6847.9199\n",
      "Epoch 52/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6789.7088 - KL_loss: 19.9466 - recon_loss: 6769.7632 - val_loss: 6716.5373 - val_KL_loss: 14.5154 - val_recon_loss: 6702.0215\n",
      "Epoch 53/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6791.5374 - KL_loss: 21.3300 - recon_loss: 6770.2070 - val_loss: 6709.0072 - val_KL_loss: 15.1870 - val_recon_loss: 6693.8193\n",
      "Epoch 54/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6759.7342 - KL_loss: 21.2565 - recon_loss: 6738.4780 - val_loss: 6747.6481 - val_KL_loss: 17.1951 - val_recon_loss: 6730.4531\n",
      "Epoch 55/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6758.6398 - KL_loss: 21.3700 - recon_loss: 6737.2695 - val_loss: 6746.3161 - val_KL_loss: 17.0710 - val_recon_loss: 6729.2446\n",
      "Epoch 56/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6774.5948 - KL_loss: 20.0528 - recon_loss: 6754.5420 - val_loss: 6751.7489 - val_KL_loss: 15.6211 - val_recon_loss: 6736.1274\n",
      "Epoch 57/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6753.5751 - KL_loss: 20.5316 - recon_loss: 6733.0439 - val_loss: 6760.2583 - val_KL_loss: 20.0863 - val_recon_loss: 6740.1719\n",
      "Epoch 58/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6743.9586 - KL_loss: 20.8799 - recon_loss: 6723.0786 - val_loss: 6742.1028 - val_KL_loss: 14.4511 - val_recon_loss: 6727.6523\n",
      "Epoch 59/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6749.9762 - KL_loss: 22.7246 - recon_loss: 6727.2515 - val_loss: 6651.8293 - val_KL_loss: 18.3361 - val_recon_loss: 6633.4932\n",
      "Epoch 60/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6763.0310 - KL_loss: 20.3940 - recon_loss: 6742.6372 - val_loss: 6858.7737 - val_KL_loss: 17.6886 - val_recon_loss: 6841.0850\n",
      "Epoch 61/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6746.6624 - KL_loss: 22.6633 - recon_loss: 6723.9985 - val_loss: 6680.0340 - val_KL_loss: 14.3592 - val_recon_loss: 6665.6753\n",
      "Epoch 62/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6753.3993 - KL_loss: 20.0247 - recon_loss: 6733.3755 - val_loss: 6806.1034 - val_KL_loss: 13.4018 - val_recon_loss: 6792.7017\n",
      "Epoch 63/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6753.6895 - KL_loss: 21.8568 - recon_loss: 6731.8330 - val_loss: 6861.4079 - val_KL_loss: 15.2041 - val_recon_loss: 6846.2041\n",
      "Epoch 64/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6746.0733 - KL_loss: 23.4131 - recon_loss: 6722.6597 - val_loss: 6699.7936 - val_KL_loss: 19.2401 - val_recon_loss: 6680.5537\n",
      "Epoch 65/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6739.5749 - KL_loss: 23.3338 - recon_loss: 6716.2412 - val_loss: 6852.4410 - val_KL_loss: 16.4840 - val_recon_loss: 6835.9570\n",
      "Epoch 66/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6752.9511 - KL_loss: 22.2139 - recon_loss: 6730.7373 - val_loss: 6718.2611 - val_KL_loss: 16.2947 - val_recon_loss: 6701.9663\n",
      "Epoch 67/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6759.6356 - KL_loss: 24.5238 - recon_loss: 6735.1118 - val_loss: 6761.4144 - val_KL_loss: 17.8050 - val_recon_loss: 6743.6094\n",
      "Epoch 68/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6779.2833 - KL_loss: 25.0782 - recon_loss: 6754.2056 - val_loss: 6819.5109 - val_KL_loss: 20.2722 - val_recon_loss: 6799.2393\n",
      "Epoch 69/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6775.1977 - KL_loss: 20.6802 - recon_loss: 6754.5176 - val_loss: 6675.8040 - val_KL_loss: 15.3475 - val_recon_loss: 6660.4565\n",
      "Epoch 70/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6750.7962 - KL_loss: 22.3283 - recon_loss: 6728.4678 - val_loss: 6725.7806 - val_KL_loss: 25.7584 - val_recon_loss: 6700.0220\n",
      "Epoch 71/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6738.9343 - KL_loss: 22.1072 - recon_loss: 6716.8267 - val_loss: 6861.0240 - val_KL_loss: 18.5095 - val_recon_loss: 6842.5146\n",
      "Epoch 72/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6730.4263 - KL_loss: 21.1948 - recon_loss: 6709.2319 - val_loss: 6791.3633 - val_KL_loss: 16.4746 - val_recon_loss: 6774.8882\n",
      "Epoch 73/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6734.5549 - KL_loss: 23.5376 - recon_loss: 6711.0181 - val_loss: 6895.2571 - val_KL_loss: 18.9360 - val_recon_loss: 6876.3213\n",
      "Epoch 74/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6724.2273 - KL_loss: 23.0822 - recon_loss: 6701.1445 - val_loss: 6743.8017 - val_KL_loss: 14.3958 - val_recon_loss: 6729.4058\n",
      "Epoch 75/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6719.6722 - KL_loss: 22.1347 - recon_loss: 6697.5376 - val_loss: 6728.6538 - val_KL_loss: 14.9687 - val_recon_loss: 6713.6851\n",
      "Epoch 76/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6735.0076 - KL_loss: 23.4924 - recon_loss: 6711.5151 - val_loss: 6802.2709 - val_KL_loss: 14.5003 - val_recon_loss: 6787.7705\n",
      "Epoch 77/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6718.0574 - KL_loss: 23.1526 - recon_loss: 6694.9048 - val_loss: 6996.3938 - val_KL_loss: 13.9783 - val_recon_loss: 6982.4160\n",
      "Epoch 78/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6717.6677 - KL_loss: 24.2043 - recon_loss: 6693.4639 - val_loss: 6815.4983 - val_KL_loss: 17.4893 - val_recon_loss: 6798.0083\n",
      "Epoch 79/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6703.4363 - KL_loss: 23.9594 - recon_loss: 6679.4771 - val_loss: 6812.7514 - val_KL_loss: 17.1709 - val_recon_loss: 6795.5796\n",
      "Epoch 80/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6707.3328 - KL_loss: 22.2195 - recon_loss: 6685.1133 - val_loss: 6725.8833 - val_KL_loss: 18.5776 - val_recon_loss: 6707.3057\n",
      "Epoch 81/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6700.6438 - KL_loss: 24.2928 - recon_loss: 6676.3516 - val_loss: 7103.7393 - val_KL_loss: 24.5575 - val_recon_loss: 7079.1821\n",
      "Epoch 82/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6762.8757 - KL_loss: 25.9834 - recon_loss: 6736.8921 - val_loss: 6831.1230 - val_KL_loss: 21.2966 - val_recon_loss: 6809.8267\n",
      "Epoch 83/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6716.3799 - KL_loss: 23.2495 - recon_loss: 6693.1309 - val_loss: 6630.8574 - val_KL_loss: 20.0417 - val_recon_loss: 6610.8164\n",
      "Epoch 84/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6716.6977 - KL_loss: 24.2195 - recon_loss: 6692.4785 - val_loss: 6856.5171 - val_KL_loss: 20.9508 - val_recon_loss: 6835.5664\n",
      "Epoch 85/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6712.4983 - KL_loss: 24.1541 - recon_loss: 6688.3433 - val_loss: 6784.6615 - val_KL_loss: 17.8231 - val_recon_loss: 6766.8379\n",
      "Epoch 86/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6706.6777 - KL_loss: 23.3865 - recon_loss: 6683.2920 - val_loss: 6699.6217 - val_KL_loss: 16.4317 - val_recon_loss: 6683.1904\n",
      "Epoch 87/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6716.8155 - KL_loss: 23.8032 - recon_loss: 6693.0117 - val_loss: 6758.8500 - val_KL_loss: 17.8984 - val_recon_loss: 6740.9517\n",
      "Epoch 88/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6703.1803 - KL_loss: 21.9465 - recon_loss: 6681.2334 - val_loss: 6857.9258 - val_KL_loss: 16.7717 - val_recon_loss: 6841.1538\n",
      "Epoch 89/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6701.3110 - KL_loss: 24.0002 - recon_loss: 6677.3110 - val_loss: 6739.0952 - val_KL_loss: 20.8821 - val_recon_loss: 6718.2134\n",
      "Epoch 90/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6692.5448 - KL_loss: 23.0575 - recon_loss: 6669.4873 - val_loss: 6753.8653 - val_KL_loss: 18.6723 - val_recon_loss: 6735.1929\n",
      "Epoch 91/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6707.9797 - KL_loss: 24.4026 - recon_loss: 6683.5767 - val_loss: 6817.9581 - val_KL_loss: 17.0616 - val_recon_loss: 6800.8970\n",
      "Epoch 92/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6721.6475 - KL_loss: 26.3978 - recon_loss: 6695.2495 - val_loss: 6760.8162 - val_KL_loss: 16.5567 - val_recon_loss: 6744.2593\n",
      "Epoch 93/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6708.3977 - KL_loss: 24.9753 - recon_loss: 6683.4233 - val_loss: 6699.5876 - val_KL_loss: 18.0858 - val_recon_loss: 6681.5015\n",
      "Epoch 94/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6707.3872 - KL_loss: 26.1172 - recon_loss: 6681.2700 - val_loss: 6600.9866 - val_KL_loss: 19.8973 - val_recon_loss: 6581.0894\n",
      "Epoch 95/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6684.3744 - KL_loss: 24.2351 - recon_loss: 6660.1396 - val_loss: 6791.7910 - val_KL_loss: 16.5792 - val_recon_loss: 6775.2119\n",
      "Epoch 96/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6683.4365 - KL_loss: 23.0846 - recon_loss: 6660.3516 - val_loss: 6644.5422 - val_KL_loss: 15.8487 - val_recon_loss: 6628.6934\n",
      "Epoch 97/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6720.4935 - KL_loss: 21.7487 - recon_loss: 6698.7451 - val_loss: 6745.2600 - val_KL_loss: 17.5879 - val_recon_loss: 6727.6719\n",
      "Epoch 98/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6740.1892 - KL_loss: 24.7435 - recon_loss: 6715.4448 - val_loss: 6722.9975 - val_KL_loss: 20.9821 - val_recon_loss: 6702.0151\n",
      "Epoch 99/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6727.8896 - KL_loss: 27.6159 - recon_loss: 6700.2739 - val_loss: 6646.7243 - val_KL_loss: 21.0464 - val_recon_loss: 6625.6782\n",
      "Epoch 100/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6701.2277 - KL_loss: 26.5096 - recon_loss: 6674.7183 - val_loss: 6719.4036 - val_KL_loss: 18.5921 - val_recon_loss: 6700.8110\n",
      "Epoch 101/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6683.0888 - KL_loss: 25.8335 - recon_loss: 6657.2559 - val_loss: 6769.1992 - val_KL_loss: 16.4630 - val_recon_loss: 6752.7354\n",
      "Epoch 102/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6704.6619 - KL_loss: 23.6539 - recon_loss: 6681.0078 - val_loss: 6767.5255 - val_KL_loss: 15.2929 - val_recon_loss: 6752.2324\n",
      "Epoch 103/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6695.9546 - KL_loss: 23.6490 - recon_loss: 6672.3057 - val_loss: 6779.5780 - val_KL_loss: 16.7004 - val_recon_loss: 6762.8774\n",
      "Epoch 104/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6688.7206 - KL_loss: 27.8495 - recon_loss: 6660.8711 - val_loss: 6660.8004 - val_KL_loss: 19.1166 - val_recon_loss: 6641.6836\n",
      "Epoch 105/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6684.7762 - KL_loss: 26.3385 - recon_loss: 6658.4385 - val_loss: 6655.6515 - val_KL_loss: 16.5760 - val_recon_loss: 6639.0757\n",
      "Epoch 106/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6685.3831 - KL_loss: 24.5007 - recon_loss: 6660.8818 - val_loss: 6706.6824 - val_KL_loss: 14.2149 - val_recon_loss: 6692.4673\n",
      "Epoch 107/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6678.1790 - KL_loss: 24.9467 - recon_loss: 6653.2324 - val_loss: 6670.5919 - val_KL_loss: 18.3837 - val_recon_loss: 6652.2080\n",
      "Epoch 108/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6693.5311 - KL_loss: 25.6159 - recon_loss: 6667.9150 - val_loss: 6777.3677 - val_KL_loss: 19.4141 - val_recon_loss: 6757.9531\n",
      "Epoch 109/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6704.7529 - KL_loss: 25.9915 - recon_loss: 6678.7612 - val_loss: 6823.4124 - val_KL_loss: 20.2165 - val_recon_loss: 6803.1958\n",
      "Epoch 110/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6711.5492 - KL_loss: 26.4706 - recon_loss: 6685.0786 - val_loss: 6943.9637 - val_KL_loss: 20.5177 - val_recon_loss: 6923.4458\n",
      "Epoch 111/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6693.8923 - KL_loss: 24.3685 - recon_loss: 6669.5239 - val_loss: 6617.3321 - val_KL_loss: 22.3803 - val_recon_loss: 6594.9517\n",
      "Epoch 112/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6689.0728 - KL_loss: 24.1141 - recon_loss: 6664.9590 - val_loss: 6793.6976 - val_KL_loss: 18.8073 - val_recon_loss: 6774.8901\n",
      "Epoch 113/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6678.2355 - KL_loss: 24.7530 - recon_loss: 6653.4829 - val_loss: 6732.0195 - val_KL_loss: 18.0013 - val_recon_loss: 6714.0186\n",
      "Epoch 114/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6680.5919 - KL_loss: 25.2504 - recon_loss: 6655.3413 - val_loss: 6607.9552 - val_KL_loss: 21.3778 - val_recon_loss: 6586.5771\n",
      "Epoch 115/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6674.0794 - KL_loss: 24.9352 - recon_loss: 6649.1440 - val_loss: 6764.6245 - val_KL_loss: 14.4442 - val_recon_loss: 6750.1807\n",
      "Epoch 116/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6672.7820 - KL_loss: 23.7168 - recon_loss: 6649.0649 - val_loss: 6760.6730 - val_KL_loss: 14.5883 - val_recon_loss: 6746.0840\n",
      "Epoch 117/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6668.0574 - KL_loss: 22.9592 - recon_loss: 6645.0986 - val_loss: 6707.8023 - val_KL_loss: 17.4562 - val_recon_loss: 6690.3457\n",
      "Epoch 118/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6689.7517 - KL_loss: 23.9713 - recon_loss: 6665.7803 - val_loss: 6779.4138 - val_KL_loss: 18.6595 - val_recon_loss: 6760.7539\n",
      "Epoch 119/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6695.6159 - KL_loss: 28.1755 - recon_loss: 6667.4404 - val_loss: 6751.2039 - val_KL_loss: 22.1985 - val_recon_loss: 6729.0054\n",
      "Epoch 120/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6674.3881 - KL_loss: 25.3725 - recon_loss: 6649.0151 - val_loss: 6807.0325 - val_KL_loss: 19.9553 - val_recon_loss: 6787.0771\n",
      "Epoch 121/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6681.1508 - KL_loss: 25.4797 - recon_loss: 6655.6719 - val_loss: 6642.7880 - val_KL_loss: 16.8305 - val_recon_loss: 6625.9570\n",
      "Epoch 122/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6674.0965 - KL_loss: 26.2852 - recon_loss: 6647.8115 - val_loss: 6716.6697 - val_KL_loss: 17.0810 - val_recon_loss: 6699.5889\n",
      "Epoch 123/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6680.3550 - KL_loss: 26.9143 - recon_loss: 6653.4409 - val_loss: 6637.2193 - val_KL_loss: 20.0229 - val_recon_loss: 6617.1958\n",
      "Epoch 124/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6676.9015 - KL_loss: 27.5011 - recon_loss: 6649.4014 - val_loss: 6638.4928 - val_KL_loss: 18.8533 - val_recon_loss: 6619.6392\n",
      "Epoch 125/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6675.4183 - KL_loss: 27.3400 - recon_loss: 6648.0776 - val_loss: 6671.8734 - val_KL_loss: 19.7353 - val_recon_loss: 6652.1377\n",
      "Epoch 126/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6674.7727 - KL_loss: 29.4032 - recon_loss: 6645.3696 - val_loss: 6684.6221 - val_KL_loss: 20.2330 - val_recon_loss: 6664.3882\n",
      "Epoch 127/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6693.5981 - KL_loss: 29.8375 - recon_loss: 6663.7603 - val_loss: 6678.0311 - val_KL_loss: 20.8997 - val_recon_loss: 6657.1313\n",
      "Epoch 128/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6688.8764 - KL_loss: 29.8092 - recon_loss: 6659.0664 - val_loss: 6872.9365 - val_KL_loss: 18.5124 - val_recon_loss: 6854.4233\n",
      "Epoch 129/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6669.1050 - KL_loss: 27.4791 - recon_loss: 6641.6260 - val_loss: 6738.4079 - val_KL_loss: 16.9011 - val_recon_loss: 6721.5063\n",
      "Epoch 130/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6647.8009 - KL_loss: 27.1343 - recon_loss: 6620.6660 - val_loss: 6678.7116 - val_KL_loss: 17.6495 - val_recon_loss: 6661.0620\n",
      "Epoch 131/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6670.0069 - KL_loss: 25.6464 - recon_loss: 6644.3608 - val_loss: 6609.9473 - val_KL_loss: 18.6314 - val_recon_loss: 6591.3159\n",
      "Epoch 132/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6675.8421 - KL_loss: 28.5708 - recon_loss: 6647.2715 - val_loss: 6578.9063 - val_KL_loss: 17.8025 - val_recon_loss: 6561.1040\n",
      "Epoch 133/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6659.1764 - KL_loss: 28.7148 - recon_loss: 6630.4624 - val_loss: 6665.7822 - val_KL_loss: 16.6994 - val_recon_loss: 6649.0825\n",
      "Epoch 134/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6666.2688 - KL_loss: 27.2297 - recon_loss: 6639.0391 - val_loss: 6601.1872 - val_KL_loss: 19.6289 - val_recon_loss: 6581.5581\n",
      "Epoch 135/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6654.9762 - KL_loss: 27.5019 - recon_loss: 6627.4741 - val_loss: 6655.4335 - val_KL_loss: 19.6783 - val_recon_loss: 6635.7554\n",
      "Epoch 136/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6649.7406 - KL_loss: 27.7360 - recon_loss: 6622.0039 - val_loss: 6707.5578 - val_KL_loss: 22.3467 - val_recon_loss: 6685.2109\n",
      "Epoch 137/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6641.7109 - KL_loss: 27.2359 - recon_loss: 6614.4751 - val_loss: 6687.0785 - val_KL_loss: 18.3927 - val_recon_loss: 6668.6855\n",
      "Epoch 138/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6644.8651 - KL_loss: 27.5029 - recon_loss: 6617.3628 - val_loss: 6638.2894 - val_KL_loss: 16.4537 - val_recon_loss: 6621.8354\n",
      "Epoch 139/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6654.3833 - KL_loss: 29.4687 - recon_loss: 6624.9150 - val_loss: 6894.9584 - val_KL_loss: 19.2440 - val_recon_loss: 6875.7139\n",
      "Epoch 140/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6672.8682 - KL_loss: 29.9820 - recon_loss: 6642.8872 - val_loss: 6773.7005 - val_KL_loss: 20.0647 - val_recon_loss: 6753.6362\n",
      "Epoch 141/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6649.7930 - KL_loss: 29.2129 - recon_loss: 6620.5796 - val_loss: 6611.4531 - val_KL_loss: 21.3602 - val_recon_loss: 6590.0923\n",
      "Epoch 142/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6644.7135 - KL_loss: 29.6068 - recon_loss: 6615.1064 - val_loss: 6841.4993 - val_KL_loss: 20.8200 - val_recon_loss: 6820.6797\n",
      "Epoch 143/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6642.8719 - KL_loss: 28.5604 - recon_loss: 6614.3110 - val_loss: 6791.9616 - val_KL_loss: 19.6820 - val_recon_loss: 6772.2798\n",
      "Epoch 144/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6675.3104 - KL_loss: 26.5454 - recon_loss: 6648.7656 - val_loss: 6637.1629 - val_KL_loss: 21.4128 - val_recon_loss: 6615.7500\n",
      "Epoch 145/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6670.5736 - KL_loss: 30.0929 - recon_loss: 6640.4810 - val_loss: 6727.4268 - val_KL_loss: 20.7073 - val_recon_loss: 6706.7192\n",
      "Epoch 146/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6657.3835 - KL_loss: 28.3790 - recon_loss: 6629.0044 - val_loss: 6561.6673 - val_KL_loss: 24.1080 - val_recon_loss: 6537.5591\n",
      "Epoch 147/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6645.2861 - KL_loss: 29.9314 - recon_loss: 6615.3550 - val_loss: 6623.5009 - val_KL_loss: 21.0922 - val_recon_loss: 6602.4092\n",
      "Epoch 148/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6661.9622 - KL_loss: 30.1126 - recon_loss: 6631.8501 - val_loss: 6584.9394 - val_KL_loss: 24.2709 - val_recon_loss: 6560.6689\n",
      "Epoch 149/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6646.5388 - KL_loss: 31.9438 - recon_loss: 6614.5952 - val_loss: 6627.4035 - val_KL_loss: 21.2848 - val_recon_loss: 6606.1191\n",
      "Epoch 150/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6639.0520 - KL_loss: 31.5007 - recon_loss: 6607.5513 - val_loss: 6551.8118 - val_KL_loss: 22.5495 - val_recon_loss: 6529.2632\n",
      "Epoch 151/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6665.9532 - KL_loss: 31.0085 - recon_loss: 6634.9443 - val_loss: 6655.0864 - val_KL_loss: 23.5566 - val_recon_loss: 6631.5298\n",
      "Epoch 152/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6641.2787 - KL_loss: 29.7492 - recon_loss: 6611.5293 - val_loss: 6596.0013 - val_KL_loss: 21.6207 - val_recon_loss: 6574.3809\n",
      "Epoch 153/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6641.4650 - KL_loss: 28.3342 - recon_loss: 6613.1304 - val_loss: 6571.1294 - val_KL_loss: 23.6060 - val_recon_loss: 6547.5239\n",
      "Epoch 154/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6641.7635 - KL_loss: 31.0624 - recon_loss: 6610.7007 - val_loss: 6766.8321 - val_KL_loss: 23.5545 - val_recon_loss: 6743.2773\n",
      "Epoch 155/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6628.7413 - KL_loss: 29.4950 - recon_loss: 6599.2456 - val_loss: 6620.7829 - val_KL_loss: 21.3987 - val_recon_loss: 6599.3838\n",
      "Epoch 156/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6634.7435 - KL_loss: 31.7656 - recon_loss: 6602.9771 - val_loss: 6607.8366 - val_KL_loss: 21.9121 - val_recon_loss: 6585.9243\n",
      "Epoch 157/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6640.1969 - KL_loss: 32.9514 - recon_loss: 6607.2456 - val_loss: 6709.1852 - val_KL_loss: 22.6297 - val_recon_loss: 6686.5552\n",
      "Epoch 158/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6641.9704 - KL_loss: 33.6305 - recon_loss: 6608.3398 - val_loss: 6629.1747 - val_KL_loss: 21.9978 - val_recon_loss: 6607.1772\n",
      "Epoch 159/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6650.0428 - KL_loss: 32.8418 - recon_loss: 6617.2007 - val_loss: 6619.3141 - val_KL_loss: 22.8601 - val_recon_loss: 6596.4541\n",
      "Epoch 160/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6640.1408 - KL_loss: 33.8951 - recon_loss: 6606.2456 - val_loss: 6637.8731 - val_KL_loss: 24.3125 - val_recon_loss: 6613.5605\n",
      "Epoch 161/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6658.2411 - KL_loss: 34.3858 - recon_loss: 6623.8555 - val_loss: 6529.4602 - val_KL_loss: 28.3877 - val_recon_loss: 6501.0728\n",
      "Epoch 162/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6629.7228 - KL_loss: 32.9185 - recon_loss: 6596.8037 - val_loss: 6578.6074 - val_KL_loss: 25.8632 - val_recon_loss: 6552.7437\n",
      "Epoch 163/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6628.6739 - KL_loss: 32.0250 - recon_loss: 6596.6494 - val_loss: 6744.6785 - val_KL_loss: 24.2016 - val_recon_loss: 6720.4771\n",
      "Epoch 164/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6624.1718 - KL_loss: 32.4659 - recon_loss: 6591.7061 - val_loss: 6644.4254 - val_KL_loss: 24.9551 - val_recon_loss: 6619.4712\n",
      "Epoch 165/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6619.6448 - KL_loss: 32.6654 - recon_loss: 6586.9800 - val_loss: 6680.7208 - val_KL_loss: 26.1723 - val_recon_loss: 6654.5483\n",
      "Epoch 166/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6656.9353 - KL_loss: 34.9133 - recon_loss: 6622.0220 - val_loss: 6573.3832 - val_KL_loss: 24.4593 - val_recon_loss: 6548.9233\n",
      "Epoch 167/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6620.2201 - KL_loss: 32.7398 - recon_loss: 6587.4810 - val_loss: 6619.2205 - val_KL_loss: 23.9300 - val_recon_loss: 6595.2905\n",
      "Epoch 168/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6623.6827 - KL_loss: 30.4474 - recon_loss: 6593.2354 - val_loss: 6840.5734 - val_KL_loss: 24.0679 - val_recon_loss: 6816.5054\n",
      "Epoch 169/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6633.2446 - KL_loss: 32.1172 - recon_loss: 6601.1270 - val_loss: 6599.0375 - val_KL_loss: 26.1543 - val_recon_loss: 6572.8838\n",
      "Epoch 170/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6623.3175 - KL_loss: 33.6392 - recon_loss: 6589.6787 - val_loss: 6599.5313 - val_KL_loss: 26.8165 - val_recon_loss: 6572.7148\n",
      "Epoch 171/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6624.1861 - KL_loss: 33.0542 - recon_loss: 6591.1318 - val_loss: 6670.6421 - val_KL_loss: 24.0148 - val_recon_loss: 6646.6270\n",
      "Epoch 172/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6608.6550 - KL_loss: 31.8272 - recon_loss: 6576.8271 - val_loss: 6651.3822 - val_KL_loss: 23.3589 - val_recon_loss: 6628.0239\n",
      "Epoch 173/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6649.2965 - KL_loss: 32.5997 - recon_loss: 6616.6978 - val_loss: 6675.9264 - val_KL_loss: 21.9727 - val_recon_loss: 6653.9541\n",
      "Epoch 174/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6641.6663 - KL_loss: 30.7836 - recon_loss: 6610.8823 - val_loss: 6543.4376 - val_KL_loss: 23.5008 - val_recon_loss: 6519.9375\n",
      "Epoch 175/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6632.3966 - KL_loss: 34.5148 - recon_loss: 6597.8809 - val_loss: 6621.4651 - val_KL_loss: 21.3241 - val_recon_loss: 6600.1406\n",
      "Epoch 176/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6614.5515 - KL_loss: 29.0118 - recon_loss: 6585.5405 - val_loss: 6603.3756 - val_KL_loss: 24.7981 - val_recon_loss: 6578.5771\n",
      "Epoch 177/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6624.1558 - KL_loss: 28.9069 - recon_loss: 6595.2490 - val_loss: 6635.2494 - val_KL_loss: 25.5461 - val_recon_loss: 6609.7031\n",
      "Epoch 178/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6614.8321 - KL_loss: 30.2867 - recon_loss: 6584.5454 - val_loss: 6605.4975 - val_KL_loss: 25.3611 - val_recon_loss: 6580.1362\n",
      "Epoch 179/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6619.9801 - KL_loss: 31.5317 - recon_loss: 6588.4487 - val_loss: 6652.1465 - val_KL_loss: 27.7215 - val_recon_loss: 6624.4253\n",
      "Epoch 180/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6606.4426 - KL_loss: 32.8554 - recon_loss: 6573.5879 - val_loss: 6615.2579 - val_KL_loss: 26.3153 - val_recon_loss: 6588.9429\n",
      "Epoch 181/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6601.6045 - KL_loss: 28.8728 - recon_loss: 6572.7305 - val_loss: 6554.6976 - val_KL_loss: 22.6663 - val_recon_loss: 6532.0312\n",
      "Epoch 182/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6642.1077 - KL_loss: 30.0059 - recon_loss: 6612.1016 - val_loss: 6642.9720 - val_KL_loss: 23.8306 - val_recon_loss: 6619.1416\n",
      "Epoch 183/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6621.8219 - KL_loss: 32.3033 - recon_loss: 6589.5181 - val_loss: 6736.5418 - val_KL_loss: 22.7678 - val_recon_loss: 6713.7744\n",
      "Epoch 184/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6611.4823 - KL_loss: 31.1464 - recon_loss: 6580.3359 - val_loss: 6798.7108 - val_KL_loss: 23.9373 - val_recon_loss: 6774.7739\n",
      "Epoch 185/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6607.0733 - KL_loss: 29.6285 - recon_loss: 6577.4448 - val_loss: 6570.1468 - val_KL_loss: 22.2057 - val_recon_loss: 6547.9419\n",
      "Epoch 186/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6602.8046 - KL_loss: 30.6092 - recon_loss: 6572.1953 - val_loss: 6682.6579 - val_KL_loss: 27.6963 - val_recon_loss: 6654.9619\n",
      "Epoch 187/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6620.9509 - KL_loss: 34.0769 - recon_loss: 6586.8745 - val_loss: 6508.2022 - val_KL_loss: 27.8092 - val_recon_loss: 6480.3931\n",
      "Epoch 188/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6598.6962 - KL_loss: 31.6305 - recon_loss: 6567.0659 - val_loss: 6698.4347 - val_KL_loss: 25.6060 - val_recon_loss: 6672.8286\n",
      "Epoch 189/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6596.4534 - KL_loss: 29.7004 - recon_loss: 6566.7539 - val_loss: 6542.2718 - val_KL_loss: 24.8990 - val_recon_loss: 6517.3726\n",
      "Epoch 190/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6598.1133 - KL_loss: 30.3858 - recon_loss: 6567.7275 - val_loss: 6616.6188 - val_KL_loss: 27.8099 - val_recon_loss: 6588.8091\n",
      "Epoch 191/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6595.6387 - KL_loss: 32.4093 - recon_loss: 6563.2290 - val_loss: 6668.4782 - val_KL_loss: 23.4881 - val_recon_loss: 6644.9907\n",
      "Epoch 192/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6611.3686 - KL_loss: 33.2271 - recon_loss: 6578.1416 - val_loss: 6580.9994 - val_KL_loss: 26.0725 - val_recon_loss: 6554.9268\n",
      "Epoch 193/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6611.0041 - KL_loss: 34.8739 - recon_loss: 6576.1309 - val_loss: 6611.1007 - val_KL_loss: 27.3086 - val_recon_loss: 6583.7920\n",
      "Epoch 194/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6619.0458 - KL_loss: 32.6265 - recon_loss: 6586.4194 - val_loss: 6568.7492 - val_KL_loss: 24.7938 - val_recon_loss: 6543.9556\n",
      "Epoch 195/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6597.8441 - KL_loss: 32.0500 - recon_loss: 6565.7944 - val_loss: 6579.3769 - val_KL_loss: 24.3952 - val_recon_loss: 6554.9814\n",
      "Epoch 196/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6598.8696 - KL_loss: 34.2530 - recon_loss: 6564.6162 - val_loss: 6595.1210 - val_KL_loss: 28.5968 - val_recon_loss: 6566.5244\n",
      "Epoch 197/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6613.0476 - KL_loss: 33.0138 - recon_loss: 6580.0342 - val_loss: 6583.6021 - val_KL_loss: 26.0521 - val_recon_loss: 6557.5503\n",
      "Epoch 198/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6611.2183 - KL_loss: 33.2758 - recon_loss: 6577.9429 - val_loss: 6643.6592 - val_KL_loss: 24.5890 - val_recon_loss: 6619.0703\n",
      "Epoch 199/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6600.0088 - KL_loss: 30.8457 - recon_loss: 6569.1626 - val_loss: 6693.9409 - val_KL_loss: 25.2473 - val_recon_loss: 6668.6943\n",
      "Epoch 200/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6600.1698 - KL_loss: 31.9152 - recon_loss: 6568.2549 - val_loss: 6804.5643 - val_KL_loss: 24.7185 - val_recon_loss: 6779.8462\n",
      "Epoch 201/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6598.2753 - KL_loss: 32.1221 - recon_loss: 6566.1533 - val_loss: 6544.7604 - val_KL_loss: 23.4884 - val_recon_loss: 6521.2715\n",
      "Epoch 202/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6604.8099 - KL_loss: 32.5528 - recon_loss: 6572.2568 - val_loss: 6549.1092 - val_KL_loss: 27.6408 - val_recon_loss: 6521.4688\n",
      "Epoch 203/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6596.2712 - KL_loss: 32.3448 - recon_loss: 6563.9263 - val_loss: 6635.2133 - val_KL_loss: 22.5539 - val_recon_loss: 6612.6592\n",
      "Epoch 204/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6605.4235 - KL_loss: 34.3047 - recon_loss: 6571.1187 - val_loss: 6601.9411 - val_KL_loss: 26.0891 - val_recon_loss: 6575.8516\n",
      "Epoch 205/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6605.8843 - KL_loss: 34.7306 - recon_loss: 6571.1543 - val_loss: 6626.7282 - val_KL_loss: 27.8685 - val_recon_loss: 6598.8594\n",
      "Epoch 206/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6591.6168 - KL_loss: 34.1666 - recon_loss: 6557.4497 - val_loss: 6551.8916 - val_KL_loss: 28.0552 - val_recon_loss: 6523.8364\n",
      "Epoch 207/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6602.2390 - KL_loss: 34.9504 - recon_loss: 6567.2876 - val_loss: 6559.2238 - val_KL_loss: 23.0875 - val_recon_loss: 6536.1367\n",
      "Epoch 208/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6599.0082 - KL_loss: 32.6951 - recon_loss: 6566.3125 - val_loss: 6589.0826 - val_KL_loss: 25.2299 - val_recon_loss: 6563.8525\n",
      "Epoch 209/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6591.8073 - KL_loss: 32.8671 - recon_loss: 6558.9409 - val_loss: 6537.5898 - val_KL_loss: 23.6903 - val_recon_loss: 6513.8989\n",
      "Epoch 210/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6605.4148 - KL_loss: 32.2329 - recon_loss: 6573.1821 - val_loss: 6684.1911 - val_KL_loss: 28.3009 - val_recon_loss: 6655.8901\n",
      "Epoch 211/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6612.6462 - KL_loss: 35.1248 - recon_loss: 6577.5215 - val_loss: 6602.2179 - val_KL_loss: 30.2311 - val_recon_loss: 6571.9863\n",
      "Epoch 212/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6584.7731 - KL_loss: 32.3671 - recon_loss: 6552.4058 - val_loss: 6537.0124 - val_KL_loss: 24.2149 - val_recon_loss: 6512.7974\n",
      "Epoch 213/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6595.1273 - KL_loss: 32.8104 - recon_loss: 6562.3174 - val_loss: 6616.8330 - val_KL_loss: 25.6235 - val_recon_loss: 6591.2095\n",
      "Epoch 214/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6581.1414 - KL_loss: 31.7963 - recon_loss: 6549.3457 - val_loss: 6530.6809 - val_KL_loss: 24.7299 - val_recon_loss: 6505.9512\n",
      "Epoch 215/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6610.4814 - KL_loss: 32.9892 - recon_loss: 6577.4927 - val_loss: 6697.4646 - val_KL_loss: 26.0653 - val_recon_loss: 6671.3989\n",
      "Epoch 216/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6589.7983 - KL_loss: 34.6095 - recon_loss: 6555.1890 - val_loss: 6552.3144 - val_KL_loss: 27.9344 - val_recon_loss: 6524.3809\n",
      "Epoch 217/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6595.7002 - KL_loss: 33.9556 - recon_loss: 6561.7446 - val_loss: 6658.6418 - val_KL_loss: 26.2395 - val_recon_loss: 6632.4023\n",
      "Epoch 218/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6598.0071 - KL_loss: 36.8369 - recon_loss: 6561.1699 - val_loss: 6580.9475 - val_KL_loss: 25.4755 - val_recon_loss: 6555.4727\n",
      "Epoch 219/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6592.2534 - KL_loss: 33.5130 - recon_loss: 6558.7397 - val_loss: 6585.9495 - val_KL_loss: 24.7192 - val_recon_loss: 6561.2300\n",
      "Epoch 220/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6584.8085 - KL_loss: 32.8160 - recon_loss: 6551.9932 - val_loss: 6594.5577 - val_KL_loss: 24.5298 - val_recon_loss: 6570.0283\n",
      "Epoch 221/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6587.9302 - KL_loss: 34.4070 - recon_loss: 6553.5239 - val_loss: 6587.1826 - val_KL_loss: 26.3435 - val_recon_loss: 6560.8389\n",
      "Epoch 222/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6624.5468 - KL_loss: 36.0154 - recon_loss: 6588.5317 - val_loss: 6526.5000 - val_KL_loss: 27.6863 - val_recon_loss: 6498.8135\n",
      "Epoch 223/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6594.1281 - KL_loss: 35.8934 - recon_loss: 6558.2349 - val_loss: 6548.3448 - val_KL_loss: 28.3754 - val_recon_loss: 6519.9692\n",
      "Epoch 224/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6592.5566 - KL_loss: 35.0718 - recon_loss: 6557.4854 - val_loss: 6559.2936 - val_KL_loss: 29.2421 - val_recon_loss: 6530.0513\n",
      "Epoch 225/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6590.6856 - KL_loss: 35.4079 - recon_loss: 6555.2773 - val_loss: 6554.3396 - val_KL_loss: 28.5104 - val_recon_loss: 6525.8296\n",
      "Epoch 226/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6583.0550 - KL_loss: 34.8104 - recon_loss: 6548.2451 - val_loss: 6523.1973 - val_KL_loss: 27.5192 - val_recon_loss: 6495.6782\n",
      "Epoch 227/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6587.6423 - KL_loss: 32.8085 - recon_loss: 6554.8335 - val_loss: 6554.7197 - val_KL_loss: 27.6000 - val_recon_loss: 6527.1201\n",
      "Epoch 228/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6599.8465 - KL_loss: 36.6697 - recon_loss: 6563.1763 - val_loss: 6549.5488 - val_KL_loss: 31.5770 - val_recon_loss: 6517.9717\n",
      "Epoch 229/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6573.4416 - KL_loss: 35.1471 - recon_loss: 6538.2939 - val_loss: 6613.6267 - val_KL_loss: 25.4833 - val_recon_loss: 6588.1436\n",
      "Epoch 230/300\n",
      "2937/2937 [==============================] - 8s 3ms/sample - loss: 6588.0283 - KL_loss: 34.2499 - recon_loss: 6553.7773 - val_loss: 6583.2220 - val_KL_loss: 28.5437 - val_recon_loss: 6554.6787\n",
      "Epoch 231/300\n",
      "1920/2937 [==================>...........] - ETA: 2s - loss: 6607.9854 - KL_loss: 34.5783 - recon_loss: 6573.4062"
     ]
    }
   ],
   "source": [
    "cvae_hist = CVAE.fit([imgs_train2, labels_train], y_train/255, verbose = 1, batch_size=batch_size, epochs=n_epoch,\n",
    "                      validation_data = ([imgs_test2, labels_test], y_test/255))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-25T13:01:14.148326236Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cvae_hist.history['loss'])\n",
    "plt.plot(cvae_hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z = np.zeros(shape=(1, latent_dim))  # latent vector\n",
    "z = np.random.normal(size=(1, latent_dim))\n",
    "label = np.array([[0, 1, 0, 0]])  # label in one hot encoding\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(decoder.predict([z, label]).reshape(128, 128), cmap=plt.cm.gray)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data science",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
