{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b184bba",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0114c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:27:38.575835876Z",
     "start_time": "2023-07-24T19:27:37.335854044Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 14:27:37.450589: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-24 14:27:37.472531: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-24 14:27:37.472946: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-24 14:27:37.889071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda, Concatenate, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "# import tensorflow.keras.backend as K\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "# import tensorflow._api.v2.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2350570",
   "metadata": {},
   "source": [
    " # CVAE simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778b296",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b6e3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:27:43.464850946Z",
     "start_time": "2023-07-24T19:27:43.315033587Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "n_pixels = np.prod(X_train.shape[1:])\n",
    "X_train = X_train.reshape((len(X_train), n_pixels))\n",
    "X_test = X_test.reshape((len(X_test), n_pixels))\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ade0a",
   "metadata": {},
   "source": [
    "## Modelamiento\n",
    "### HiperparÃ¡metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1a0ffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:27:44.780689197Z",
     "start_time": "2023-07-24T19:27:44.776864160Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 250 # batch size\n",
    "latent_dim = 2 # latent space size\n",
    "optim = Adam(lr=0.001)\n",
    "n_x = X_train.shape[1]\n",
    "n_y = y_train.shape[1]\n",
    "n_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64eb223",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167bdde6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:27:46.170949843Z",
     "start_time": "2023-07-24T19:27:46.128631995Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inp1 = Input(shape=(n_x,), name=\"input_image\")\n",
    "encoder_inp2 = Input(shape=(n_y,), name=\"input_label\")\n",
    "enc_concat = Concatenate(name=\"encoder_concatenate\")([encoder_inp1, encoder_inp2])\n",
    "encoder_hidden = Dense(512, name=\"hidden_layer\")(enc_concat)\n",
    "mu = Dense(latent_dim, activation='linear', name=\"mu\")(encoder_hidden)\n",
    "l_sigma = Dense(latent_dim, activation='linear', name=\"l_sigma\")(encoder_hidden)\n",
    "def sample_z(args):\n",
    "    mu, l_sigma = args\n",
    "    eps = K.random_normal(shape=(latent_dim, ), mean=0., stddev=1.) # shape antes => (batch_size, latent_dim)\n",
    "    return mu + K.exp(l_sigma / 2) * eps\n",
    "z = Lambda(sample_z, output_shape = (latent_dim, ), name=\"latent_vector\")([mu, l_sigma]) # output encoder\n",
    "\n",
    "encoder = Model([encoder_inp1, encoder_inp2], z, name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_image (InputLayer)    [(None, 784)]                0         []                            \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " encoder_concatenate (Conca  (None, 794)                  0         ['input_image[0][0]',         \n",
      " tenate)                                                             'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)        (None, 512)                  407040    ['encoder_concatenate[0][0]'] \n",
      "                                                                                                  \n",
      " mu (Dense)                  (None, 2)                    1026      ['hidden_layer[0][0]']        \n",
      "                                                                                                  \n",
      " l_sigma (Dense)             (None, 2)                    1026      ['hidden_layer[0][0]']        \n",
      "                                                                                                  \n",
      " latent_vector (Lambda)      (None, 2)                    0         ['mu[0][0]',                  \n",
      "                                                                     'l_sigma[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 409092 (1.56 MB)\n",
      "Trainable params: 409092 (1.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T19:28:11.564417419Z",
     "start_time": "2023-07-24T19:28:11.517475434Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "7875777a",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148b2dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:29:34.296378522Z",
     "start_time": "2023-07-24T19:29:34.249766211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_latent_vector (Input  [(None, 2)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " decoder_concat (Concatenat  (None, 12)                   0         ['input_latent_vector[0][0]', \n",
      " e)                                                                  'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)        (None, 512)                  6656      ['decoder_concat[0][0]']      \n",
      "                                                                                                  \n",
      " output_img (Dense)          (None, 784)                  402192    ['hidden_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408848 (1.56 MB)\n",
      "Trainable params: 408848 (1.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inp1 = Input(shape=(latent_dim), name=\"input_latent_vector\")\n",
    "decoder_inp2 = Input(shape=(n_y), name = \"input_label\")\n",
    "dec_concat = Concatenate(name=\"decoder_concat\")([decoder_inp1, decoder_inp2]) \n",
    "decoder_hidden = Dense(512, activation=\"relu\", name=\"hidden_layer\")(dec_concat)\n",
    "output = Dense(n_x, activation=\"sigmoid\", name=\"output_img\")(decoder_hidden) # output img\n",
    "decoder = Model([decoder_inp1, decoder_inp2], output, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e48ae",
   "metadata": {},
   "source": [
    "### Conditional variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5672485c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:30:56.616316764Z",
     "start_time": "2023-07-24T19:30:56.574813439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CVAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_image (InputLayer)    [(None, 784)]                0         []                            \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        (None, 2)                    409092    ['input_image[0][0]',         \n",
      "                                                                     'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 784)                  408848    ['encoder[0][0]',             \n",
      "                                                                     'input_label[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 817940 (3.12 MB)\n",
      "Trainable params: 817940 (3.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_vector = encoder(inputs=[encoder_inp1, encoder_inp2]) # z\n",
    "output_img = decoder(inputs=[latent_vector, encoder_inp2])\n",
    "CVAE = Model(inputs=[encoder_inp1, encoder_inp2], outputs=output_img, name=\"CVAE\")\n",
    "CVAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff524a",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9d492a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:31:15.719988055Z",
     "start_time": "2023-07-24T19:31:15.678398401Z"
    }
   },
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    recon = recon_loss(y_true, y_pred)\n",
    "    kl = KL_loss(y_true, y_pred)\n",
    "    return recon + kl\n",
    "\n",
    "def KL_loss(y_true, y_pred): # it doesnt use y_true and y_pred but the parameters are necessary for compiling\n",
    "     # importante se usan las capas de mu y sigma del encoder\n",
    "\treturn(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))\n",
    "\n",
    "def recon_loss(y_true, y_pred):\n",
    "\treturn K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6dddcdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:31:18.180885285Z",
     "start_time": "2023-07-24T19:31:18.109926059Z"
    }
   },
   "outputs": [],
   "source": [
    "CVAE.compile(optimizer=optim, loss=vae_loss, metrics = [KL_loss, recon_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10db0db9",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T19:31:51.819898471Z",
     "start_time": "2023-07-24T19:31:19.699999794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " 2750/60000 [>.............................] - ETA: 2s - loss: 461.1898 - KL_loss: 32.0418 - recon_loss: 429.1480"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 14:31:19.807275: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-07-24 14:31:19.807296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: mordor\n",
      "2023-07-24 14:31:19.807299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: mordor\n",
      "2023-07-24 14:31:19.807359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.116.4\n",
      "2023-07-24 14:31:19.807370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.116.4\n",
      "2023-07-24 14:31:19.807372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.116.4\n",
      "2023-07-24 14:31:19.816872: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-07-24 14:31:19.833023: W tensorflow/c/c_api.cc:304] Operation '{name:'training/Adam/hidden_layer_1/bias/m/Assign' id:563 op device:{requested: '', assigned: ''} def:{{{node training/Adam/hidden_layer_1/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/hidden_layer_1/bias/m, training/Adam/hidden_layer_1/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 22us/sample - loss: 202.5425 - KL_loss: 10.9922 - recon_loss: 191.5504 - val_loss: 161.0123 - val_KL_loss: 5.1330 - val_recon_loss: 155.8793\n",
      "Epoch 2/50\n",
      " 2750/60000 [>.............................] - ETA: 1s - loss: 159.2400 - KL_loss: 4.7063 - recon_loss: 154.5337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 14:31:21.108865: W tensorflow/c/c_api.cc:304] Operation '{name:'loss/mul' id:278 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 20us/sample - loss: 155.5990 - KL_loss: 4.7612 - recon_loss: 150.8379 - val_loss: 150.4927 - val_KL_loss: 4.1087 - val_recon_loss: 146.3840\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 149.9958 - KL_loss: 4.5659 - recon_loss: 145.4299 - val_loss: 146.7016 - val_KL_loss: 4.1735 - val_recon_loss: 142.5281\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 147.6305 - KL_loss: 4.4763 - recon_loss: 143.1543 - val_loss: 145.0735 - val_KL_loss: 4.9209 - val_recon_loss: 140.1525\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 145.4643 - KL_loss: 4.3408 - recon_loss: 141.1234 - val_loss: 144.6571 - val_KL_loss: 4.3490 - val_recon_loss: 140.3081\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 144.1153 - KL_loss: 4.1448 - recon_loss: 139.9705 - val_loss: 143.7066 - val_KL_loss: 3.9015 - val_recon_loss: 139.8051\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 143.8941 - KL_loss: 4.2554 - recon_loss: 139.6387 - val_loss: 144.3173 - val_KL_loss: 3.3682 - val_recon_loss: 140.9491\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 143.3073 - KL_loss: 4.3880 - recon_loss: 138.9192 - val_loss: 143.1289 - val_KL_loss: 3.8388 - val_recon_loss: 139.2901\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 142.8885 - KL_loss: 4.3428 - recon_loss: 138.5456 - val_loss: 142.2422 - val_KL_loss: 4.3021 - val_recon_loss: 137.9401\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 142.5812 - KL_loss: 4.2863 - recon_loss: 138.2949 - val_loss: 141.5917 - val_KL_loss: 4.1018 - val_recon_loss: 137.4899\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 142.0858 - KL_loss: 4.3497 - recon_loss: 137.7361 - val_loss: 141.2912 - val_KL_loss: 4.6209 - val_recon_loss: 136.6703\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 141.8031 - KL_loss: 4.3847 - recon_loss: 137.4185 - val_loss: 141.9954 - val_KL_loss: 3.3287 - val_recon_loss: 138.6667\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 141.5341 - KL_loss: 4.3069 - recon_loss: 137.2272 - val_loss: 141.2518 - val_KL_loss: 4.5205 - val_recon_loss: 136.7313\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 141.4368 - KL_loss: 4.2768 - recon_loss: 137.1601 - val_loss: 141.0913 - val_KL_loss: 4.1501 - val_recon_loss: 136.9412\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 141.5218 - KL_loss: 4.4571 - recon_loss: 137.0647 - val_loss: 141.5620 - val_KL_loss: 4.8875 - val_recon_loss: 136.6745\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 141.0560 - KL_loss: 4.4352 - recon_loss: 136.6207 - val_loss: 141.1050 - val_KL_loss: 3.9361 - val_recon_loss: 137.1688\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 140.6227 - KL_loss: 4.2114 - recon_loss: 136.4112 - val_loss: 140.0860 - val_KL_loss: 3.9650 - val_recon_loss: 136.1210\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 141.2119 - KL_loss: 4.5276 - recon_loss: 136.6843 - val_loss: 140.5159 - val_KL_loss: 4.6811 - val_recon_loss: 135.8348\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 140.7152 - KL_loss: 4.4060 - recon_loss: 136.3092 - val_loss: 140.4983 - val_KL_loss: 4.2775 - val_recon_loss: 136.2208\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 140.6258 - KL_loss: 4.3454 - recon_loss: 136.2804 - val_loss: 140.1941 - val_KL_loss: 4.5561 - val_recon_loss: 135.6380\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 140.4085 - KL_loss: 4.4083 - recon_loss: 136.0002 - val_loss: 139.7515 - val_KL_loss: 3.8282 - val_recon_loss: 135.9232\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 140.5716 - KL_loss: 4.5616 - recon_loss: 136.0100 - val_loss: 142.7745 - val_KL_loss: 3.5400 - val_recon_loss: 139.2344\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 140.2352 - KL_loss: 4.3685 - recon_loss: 135.8667 - val_loss: 140.2535 - val_KL_loss: 4.9007 - val_recon_loss: 135.3528\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 140.1928 - KL_loss: 4.3758 - recon_loss: 135.8170 - val_loss: 141.8321 - val_KL_loss: 4.2723 - val_recon_loss: 137.5599\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 140.2077 - KL_loss: 4.4292 - recon_loss: 135.7785 - val_loss: 140.5399 - val_KL_loss: 5.6513 - val_recon_loss: 134.8886\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 140.0671 - KL_loss: 4.5206 - recon_loss: 135.5465 - val_loss: 140.6673 - val_KL_loss: 3.9817 - val_recon_loss: 136.6856\n"
     ]
    }
   ],
   "source": [
    "cvae_hist = CVAE.fit([X_train, y_train], X_train, verbose = 1, batch_size=batch_size, epochs=n_epoch,\n",
    "                      validation_data = ([X_test, y_test], X_test),\n",
    "                      callbacks = [EarlyStopping(patience = 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55068890",
   "metadata": {},
   "source": [
    "## PredicciÃ³n\n",
    "\n",
    "Por defecto utilizamos un vector latente de ceros. Para variar el output se puede cambiar el vector latente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45b53e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:33:12.055983277Z",
     "start_time": "2023-07-24T19:33:12.011387639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 300x300 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.zeros(shape=(1, latent_dim)) # latent vector\n",
    "label = np.array([[0,1,0,0,0,0,0,0,0,0]]) # label in one hot encoding\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(decoder.predict([z, label]).reshape(28,28), cmap = plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744d4bf",
   "metadata": {},
   "source": [
    " # CVAE con capas convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f9dff",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d3a5384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:33:23.865605153Z",
     "start_time": "2023-07-24T19:33:23.716633942Z"
    }
   },
   "outputs": [],
   "source": [
    "(imgs_train, labels_train), (imgs_test, labels_test) = mnist.load_data()\n",
    "imgs_train = (imgs_train.astype('float32') / 255)[:, :, :, np.newaxis]\n",
    "imgs_test = (imgs_test.astype('float32') / 255)[:, :, :, np.newaxis]\n",
    "\n",
    "labels_train = to_categorical(labels_train)\n",
    "labels_test = to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (28, 28, 1)\n",
      "n_cat shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "print(\"image shape:\", imgs_train.shape[1:])\n",
    "print(\"n_cat shape:\", labels_train.shape[1:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T19:33:25.671627479Z",
     "start_time": "2023-07-24T19:33:25.667606094Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "bddfe036",
   "metadata": {},
   "source": [
    "## Modelamiento\n",
    "### HiperparÃ¡metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf290c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:33:39.991935947Z",
     "start_time": "2023-07-24T19:33:39.950783339Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 250 # batch size\n",
    "latent_dim = 2 # latent space size\n",
    "optim = Adam(lr=0.001)\n",
    "shape_img = imgs_train.shape[1:]\n",
    "n_cat = labels_train.shape[1]\n",
    "n_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038f4d2",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6213ce87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:36:45.952279824Z",
     "start_time": "2023-07-24T19:36:45.869835443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_image (InputLayer)    [(None, 28, 28, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 13, 13, 32)           320       ['input_image[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 6, 6, 64)             18496     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 2304)                 0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 2314)                 0         ['flatten_1[0][0]',           \n",
      "                                                                     'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)        (None, 512)                  1185280   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " mu (Dense)                  (None, 2)                    1026      ['hidden_layer[0][0]']        \n",
      "                                                                                                  \n",
      " l_sigma (Dense)             (None, 2)                    1026      ['hidden_layer[0][0]']        \n",
      "                                                                                                  \n",
      " latent_vector (Lambda)      (None, 2)                    0         ['mu[0][0]',                  \n",
      "                                                                     'l_sigma[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1206148 (4.60 MB)\n",
      "Trainable params: 1206148 (4.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_input_image = Input(shape=(shape_img), name=\"input_image\")\n",
    "enc_input_label = Input(shape=(n_cat), name=\"input_label\")\n",
    "\n",
    "# convolutionals block\n",
    "conv1 = Conv2D(filters=32, kernel_size=3, strides=2, activation=\"relu\")(enc_input_image)\n",
    "conv2 = Conv2D(filters=64, kernel_size=3, strides=2, activation='relu')(conv1)\n",
    "\n",
    "# concat\n",
    "flattened = Flatten()(conv2)\n",
    "enc_concat = Concatenate()([flattened, enc_input_label])\n",
    "\n",
    "encoder_hidden = Dense(512, name=\"hidden_layer\")(enc_concat)\n",
    "mu = Dense(latent_dim, activation='linear', name=\"mu\")(encoder_hidden)\n",
    "l_sigma = Dense(latent_dim, activation='linear', name=\"l_sigma\")(encoder_hidden)\n",
    "def sample_z(args):\n",
    "    mu, l_sigma = args\n",
    "    eps = K.random_normal(shape=(latent_dim, ), mean=0., stddev=1.) # shape antes => (batch_size, latent_dim)\n",
    "    return mu + K.exp(l_sigma / 2) * eps\n",
    "z = Lambda(sample_z, output_shape = (latent_dim, ), name=\"latent_vector\")([mu, l_sigma]) # output encoder\n",
    "\n",
    "encoder = Model([enc_input_image, enc_input_label], z, name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f82df",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "779e58de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:36:47.032654157Z",
     "start_time": "2023-07-24T19:36:46.941069035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_latent_vector (Input  [(None, 2)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " decoder_concat (Concatenat  (None, 12)                   0         ['input_latent_vector[0][0]', \n",
      " e)                                                                  'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)        (None, 512)                  6656      ['decoder_concat[0][0]']      \n",
      "                                                                                                  \n",
      " hidden_layer2 (Dense)       (None, 1568)                 804384    ['hidden_layer[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 7, 7, 32)             0         ['hidden_layer2[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 14, 14, 64)           18496     ['reshape_1[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2D  (None, 28, 28, 32)           18464     ['conv2d_transpose_3[0][0]']  \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2D  (None, 28, 28, 1)            289       ['conv2d_transpose_4[0][0]']  \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)         (None, 784)                  0         ['conv2d_transpose_5[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 848289 (3.24 MB)\n",
      "Trainable params: 848289 (3.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec_inp_latent_vector = Input(shape=(latent_dim), name=\"input_latent_vector\")\n",
    "decoder_inp_label = Input(shape=(n_cat), name = \"input_label\")\n",
    "dec_concat = Concatenate(name=\"decoder_concat\")([dec_inp_latent_vector, decoder_inp_label])\n",
    "\n",
    "decoder_hidden = Dense(512, activation=\"relu\", name=\"hidden_layer\")(dec_concat)\n",
    "decoder_hidden = Dense(7*7*32, activation=\"relu\", name=\"hidden_layer2\")(decoder_hidden)\n",
    "reshaped = Reshape(target_shape=(7,7,32))(decoder_hidden)\n",
    "\n",
    "# convolutionals block\n",
    "dec_conv1 = Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation=\"relu\", padding='same')(reshaped)\n",
    "dec_conv2 = Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation=\"relu\", padding='same')(dec_conv1)\n",
    "dec_output_img = Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same')(dec_conv2)\n",
    "dec_output_img = Flatten()(dec_output_img)\n",
    "decoder = Model([dec_inp_latent_vector, decoder_inp_label], dec_output_img, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ae3e3",
   "metadata": {},
   "source": [
    "### Conditional Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21647254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:36:48.096180086Z",
     "start_time": "2023-07-24T19:36:48.028028032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CVAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_image (InputLayer)    [(None, 28, 28, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " input_label (InputLayer)    [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        (None, 2)                    1206148   ['input_image[0][0]',         \n",
      "                                                                     'input_label[0][0]']         \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 784)                  848289    ['encoder[0][0]',             \n",
      "                                                                     'input_label[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2054437 (7.84 MB)\n",
      "Trainable params: 2054437 (7.84 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_vector = encoder(inputs=[enc_input_image, enc_input_label]) # z\n",
    "output_img = decoder(inputs=[latent_vector, enc_input_label])\n",
    "CVAE = Model(inputs=[enc_input_image, enc_input_label], outputs=output_img, name=\"CVAE\")\n",
    "CVAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f6aa1",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "317a581d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:36:56.413400997Z",
     "start_time": "2023-07-24T19:36:56.408795362Z"
    }
   },
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    recon = recon_loss(y_true, y_pred)\n",
    "    kl = KL_loss(y_true, y_pred)\n",
    "    return recon + kl\n",
    "\n",
    "def KL_loss(y_true, y_pred): # it doesnt use y_true and y_pred but the parameters are necessary for compiling\n",
    "     # importante se usan las capas de mu y sigma del encoder\n",
    "    return(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))\n",
    "\n",
    "def recon_loss(y_true, y_pred):\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db01a324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:36:58.086082234Z",
     "start_time": "2023-07-24T19:36:58.046088070Z"
    }
   },
   "outputs": [],
   "source": [
    "CVAE.compile(optimizer=optim, loss=vae_loss, metrics=[KL_loss, recon_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d053a872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T19:37:45.653132754Z",
     "start_time": "2023-07-24T19:37:45.606952361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# the output of the decoder is a flattened img, \n",
    "# so we need to flatten the true values (input images from (28,28,1) => 784)\n",
    "y_train = imgs_train.reshape(imgs_train.shape[0], -1)\n",
    "y_test = imgs_test.reshape(imgs_test.shape[0], -1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a29281cb",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T19:44:51.153115829Z",
     "start_time": "2023-07-24T19:38:03.683532109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 14:38:03.908778: W tensorflow/c/c_api.cc:304] Operation '{name:'training_2/Adam/conv2d_transpose_5/kernel/v/Assign' id:2026 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/conv2d_transpose_5/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/conv2d_transpose_5/kernel/v, training_2/Adam/conv2d_transpose_5/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 0s - loss: 211.5188 - KL_loss: 3.3711 - recon_loss: 208.1476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 14:38:20.227075: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_1/mul' id:1461 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 17s 287us/sample - loss: 211.5188 - KL_loss: 3.3711 - recon_loss: 208.1476 - val_loss: 163.4804 - val_KL_loss: 3.7319 - val_recon_loss: 159.7484\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 17s 280us/sample - loss: 159.5977 - KL_loss: 3.3337 - recon_loss: 156.2641 - val_loss: 152.6196 - val_KL_loss: 3.4018 - val_recon_loss: 149.2178\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 17s 280us/sample - loss: 149.4043 - KL_loss: 3.5902 - recon_loss: 145.8141 - val_loss: 146.1235 - val_KL_loss: 3.1574 - val_recon_loss: 142.9661\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 147.0586 - KL_loss: 3.8095 - recon_loss: 143.2491 - val_loss: 153.9128 - val_KL_loss: 3.3044 - val_recon_loss: 150.6085\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 145.8171 - KL_loss: 4.0380 - recon_loss: 141.7791 - val_loss: 143.5939 - val_KL_loss: 4.1873 - val_recon_loss: 139.4066\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 143.5091 - KL_loss: 4.0349 - recon_loss: 139.4742 - val_loss: 143.1343 - val_KL_loss: 3.7792 - val_recon_loss: 139.3551\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 17s 284us/sample - loss: 143.9342 - KL_loss: 3.9257 - recon_loss: 140.0085 - val_loss: 143.9901 - val_KL_loss: 3.8067 - val_recon_loss: 140.1833\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 17s 284us/sample - loss: 142.2269 - KL_loss: 4.0481 - recon_loss: 138.1788 - val_loss: 140.2971 - val_KL_loss: 4.3074 - val_recon_loss: 135.9897\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 17s 287us/sample - loss: 140.0812 - KL_loss: 4.1469 - recon_loss: 135.9343 - val_loss: 140.3529 - val_KL_loss: 4.3395 - val_recon_loss: 136.0134\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 139.9812 - KL_loss: 4.1057 - recon_loss: 135.8755 - val_loss: 141.3476 - val_KL_loss: 4.2367 - val_recon_loss: 137.1109\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 17s 286us/sample - loss: 138.7439 - KL_loss: 4.4135 - recon_loss: 134.3304 - val_loss: 138.8779 - val_KL_loss: 4.4968 - val_recon_loss: 134.3812\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 17s 284us/sample - loss: 138.5881 - KL_loss: 4.3806 - recon_loss: 134.2075 - val_loss: 138.8193 - val_KL_loss: 4.3551 - val_recon_loss: 134.4643\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 17s 284us/sample - loss: 151.7759 - KL_loss: 3.7906 - recon_loss: 147.9853 - val_loss: 141.9867 - val_KL_loss: 4.4887 - val_recon_loss: 137.4980\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 140.0152 - KL_loss: 4.2006 - recon_loss: 135.8146 - val_loss: 140.3720 - val_KL_loss: 4.0972 - val_recon_loss: 136.2748\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 17s 283us/sample - loss: 137.9656 - KL_loss: 4.2283 - recon_loss: 133.7372 - val_loss: 137.2911 - val_KL_loss: 4.4684 - val_recon_loss: 132.8227\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 17s 283us/sample - loss: 138.4051 - KL_loss: 4.2451 - recon_loss: 134.1599 - val_loss: 136.7213 - val_KL_loss: 3.9239 - val_recon_loss: 132.7974\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 17s 282us/sample - loss: 136.5412 - KL_loss: 4.3263 - recon_loss: 132.2149 - val_loss: 138.6229 - val_KL_loss: 4.4013 - val_recon_loss: 134.2216\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 17s 283us/sample - loss: 136.0693 - KL_loss: 4.4556 - recon_loss: 131.6137 - val_loss: 137.5713 - val_KL_loss: 4.8460 - val_recon_loss: 132.7254\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 17s 281us/sample - loss: 136.5693 - KL_loss: 4.3861 - recon_loss: 132.1833 - val_loss: 136.0441 - val_KL_loss: 3.9396 - val_recon_loss: 132.1045\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 17s 283us/sample - loss: 136.0050 - KL_loss: 4.3607 - recon_loss: 131.6443 - val_loss: 136.2778 - val_KL_loss: 4.6864 - val_recon_loss: 131.5914\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 17s 285us/sample - loss: 135.7629 - KL_loss: 4.3475 - recon_loss: 131.4154 - val_loss: 138.6379 - val_KL_loss: 3.5914 - val_recon_loss: 135.0465\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 17s 283us/sample - loss: 138.9218 - KL_loss: 4.3988 - recon_loss: 134.5231 - val_loss: 136.2502 - val_KL_loss: 4.1038 - val_recon_loss: 132.1464\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 17s 279us/sample - loss: 135.7249 - KL_loss: 4.4654 - recon_loss: 131.2595 - val_loss: 136.3156 - val_KL_loss: 4.4165 - val_recon_loss: 131.8991\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 17s 278us/sample - loss: 136.3744 - KL_loss: 4.3932 - recon_loss: 131.9812 - val_loss: 140.2629 - val_KL_loss: 3.9872 - val_recon_loss: 136.2757\n"
     ]
    }
   ],
   "source": [
    "cvae_hist = CVAE.fit([imgs_train, labels_train], y_train, verbose = 1, batch_size=batch_size, epochs=n_epoch,\n",
    "                      validation_data = ([imgs_test, labels_test], y_test),\n",
    "                      callbacks = [EarlyStopping(patience = 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediccion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 17:19:37.604714: W tensorflow/c/c_api.cc:304] Operation '{name:'flatten_2/Reshape' id:1225 op device:{requested: '', assigned: ''} def:{{{node flatten_2/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _has_manual_control_dependencies=true](conv2d_transpose_5/BiasAdd, flatten_2/Const)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 300x300 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.zeros(shape=(1, latent_dim))  # latent vector\n",
    "label = np.array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])  # label in one hot encoding\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(decoder.predict([z, label]).reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:19:37.700232054Z",
     "start_time": "2023-07-24T22:19:37.604958230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ds",
   "language": "python",
   "display_name": "data science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
